{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translator_done_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NFuUYKHABlD"
      },
      "source": [
        "# Translations 2.0 - Neural Machine Translation\n",
        "\n",
        "According to the Google paper [*Attention is all you need*](https://arxiv.org/abs/1706.03762), you only need layers of Attention to make a Deep Learning model understand the complexity of a sentence. We will try to implement this type of model for our translator. \n",
        "\n",
        "## Project description \n",
        "\n",
        " \n",
        "\n",
        "Our data can be found on this link: https://go.aws/38ECHUB\n",
        "\n",
        "### Preprocessing \n",
        "\n",
        "The whole purpose of your preprocessing is to express your (French) entry sentence in a sequence of clues.\n",
        "\n",
        "i.e. :\n",
        "\n",
        "* je suis malade---> `[123, 21, 34, 0, 0, 0, 0, 0]`\n",
        "\n",
        "This gives a *shape* -> `(batch_size, max_len_of_a_sentence)`.\n",
        "\n",
        "The clues correspond to a number that you will have to assign for each word token. \n",
        "\n",
        "The zeros correspond to what are called [*padded_sequences*](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) which allow all word sequences to have the same length (mandatory for your algorithm). \n",
        "\n",
        "This time, we won't have to *one hot encoder* your target variable. We  will simply be able to create a vector similar to your input sentence. \n",
        "\n",
        "i.e. : \n",
        "\n",
        "* I am sick ---> `[43, 2, 42, 0, 0]`\n",
        "\n",
        "WARNING, we  will however need to add a step in our preprocessing. For each sentence we will need to add a token `<start>` & `<end>` to indicate the beginning and end of a sentence. We can do this via `Spacy`.\n",
        "\n",
        "We will use : \n",
        "\n",
        "* `Pandas` or `Numpy` for reading the text file.\n",
        "* `Spacy` for Tokenization \n",
        "* `Tensorflow` for [padded_sequence](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) \n",
        "\n",
        "### Modeling \n",
        "\n",
        "For modeling, we will need to set up layers of attention. We will need to: \n",
        "\n",
        "* Create an `Encoder` class that inherits from `tf.keras.Model`.\n",
        "* Create a Bahdanau Attention Layer that will be a class that inherits `tf.keras.layers.Layer`\n",
        "* Finally create a `Decoder` class that inherits from `tf.keras.Model`.\n",
        "\n",
        "\n",
        "We will need to create your own cost function as well as our own training loop. \n",
        "\n",
        "\n",
        "### Tips \n",
        "\n",
        "We will not take the whole dataset at the beginning for our experiments, we just take 5000 or even 3000 sentences. This will allow us to iterate faster and avoid bugs simply related to your need for computing power. \n",
        "\n",
        "Also, we acknowledge the inspiration from the [Neural Machine Translation with Attention] tutorial (https://www.tensorflow.org/tutorials/text/nmt_with_attention) from TensorFlow. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MAiXgYbAVLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997cc314-875d-4983-81dc-ec0c743fce68"
      },
      "source": [
        "!pip install --upgrade tensorflow "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (51.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qUhyNPnhBtk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0caafa86-9119-4a07-bee4-40f6ff095fe6"
      },
      "source": [
        "# Import necessaries librairies\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf \n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSWse8KgHmQw"
      },
      "source": [
        "## Import datas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2-Sd6lq_8ax"
      },
      "source": [
        "# Loading function for txt document\n",
        "def load_doc(url):\n",
        "  df = pd.read_csv(\"https://go.aws/38ECHUB\", delimiter=\"\\t\", header=None)\n",
        "  return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj34GLiihGw1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8d6c32c0-9f6f-4612-ce51-1f003cf54405"
      },
      "source": [
        "# Loading txt document\n",
        "doc = load_doc(\"https://go.aws/38ECHUB\")\n",
        "doc.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0           1\n",
              "0   Go.        Va !\n",
              "1   Hi.     Salut !\n",
              "2  Run!     Cours !\n",
              "3  Run!    Courez !\n",
              "4  Wow!  Ça alors !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jiXcCi0FFSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa70ef15-63dc-4fba-8c13-fd2adc3a228f"
      },
      "source": [
        "len(doc)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "160538"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrcFSfBZMuQ7"
      },
      "source": [
        "# Let's just take a sample of 5000 sentences to avoid slowness. \n",
        "doc = doc.sample(50000)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z1Ih3M2jVr_"
      },
      "source": [
        "# Add a <start> and <end> token \n",
        "def begin_end_sentence(sentence):\n",
        "  sentence = \"<start> \"+ sentence + \" <end>\"\n",
        "  return sentence"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVMl6744jmrq"
      },
      "source": [
        "# Add <start> and <end> token\n",
        "doc.iloc[:, 0] = doc.iloc[:, 0].apply(lambda x: begin_end_sentence(x))\n",
        "doc.iloc[:, 1] = doc.iloc[:, 1].apply(lambda x: begin_end_sentence(x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2fZfDCqFPko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "3ea67bc0-51d8-4930-d73b-f6fa1f92163b"
      },
      "source": [
        "doc"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18344</th>\n",
              "      <td>&lt;start&gt; I want to be safe. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Je veux être en sécurité. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48108</th>\n",
              "      <td>&lt;start&gt; Where are your manners? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Qu'avez-vous fait de vos bonnes manièr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24273</th>\n",
              "      <td>&lt;start&gt; The glass is empty. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Le verre est vide. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104113</th>\n",
              "      <td>&lt;start&gt; I agree with what you've written. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Je suis d'accord avec ce que tu as écr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50387</th>\n",
              "      <td>&lt;start&gt; I don't miss you at all. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Vous ne me manquez pas du tout. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92427</th>\n",
              "      <td>&lt;start&gt; Did the storm cause any damage? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Est-ce que la tempête a causé des dégâ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115141</th>\n",
              "      <td>&lt;start&gt; She is very thoughtful and patient. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Elle est vraiment attentive et patient...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142595</th>\n",
              "      <td>&lt;start&gt; Selfie sticks are not allowed in this ...</td>\n",
              "      <td>&lt;start&gt; Les perches à selfie ne sont pas autor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94975</th>\n",
              "      <td>&lt;start&gt; I'm training for the triathlon. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Je m'entraîne pour le triathlon. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75903</th>\n",
              "      <td>&lt;start&gt; I found this under your bed. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; J'ai trouvé ceci sous votre lit. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        0                                                  1\n",
              "18344                    <start> I want to be safe. <end>            <start> Je veux être en sécurité. <end>\n",
              "48108               <start> Where are your manners? <end>  <start> Qu'avez-vous fait de vos bonnes manièr...\n",
              "24273                   <start> The glass is empty. <end>                   <start> Le verre est vide. <end>\n",
              "104113    <start> I agree with what you've written. <end>  <start> Je suis d'accord avec ce que tu as écr...\n",
              "50387              <start> I don't miss you at all. <end>      <start> Vous ne me manquez pas du tout. <end>\n",
              "...                                                   ...                                                ...\n",
              "92427       <start> Did the storm cause any damage? <end>  <start> Est-ce que la tempête a causé des dégâ...\n",
              "115141  <start> She is very thoughtful and patient. <end>  <start> Elle est vraiment attentive et patient...\n",
              "142595  <start> Selfie sticks are not allowed in this ...  <start> Les perches à selfie ne sont pas autor...\n",
              "94975       <start> I'm training for the triathlon. <end>     <start> Je m'entraîne pour le triathlon. <end>\n",
              "75903          <start> I found this under your bed. <end>     <start> J'ai trouvé ceci sous votre lit. <end>\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQQOU7OI7-xV"
      },
      "source": [
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwCN5z21xo5H"
      },
      "source": [
        "tokenizer_en.fit_on_texts(doc.iloc[:,0])\n",
        "tokenizer_fr.fit_on_texts(doc.iloc[:,1])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuidpesnFjD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3eca6bb-865f-4d72-8875-501f8485838c"
      },
      "source": [
        "tokenizer_en.word_index"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<start>': 1,\n",
              " '<end>': 2,\n",
              " 'i': 3,\n",
              " 'you': 4,\n",
              " 'to': 5,\n",
              " 'the': 6,\n",
              " 'a': 7,\n",
              " 'is': 8,\n",
              " 'tom': 9,\n",
              " 'he': 10,\n",
              " 'that': 11,\n",
              " 'it': 12,\n",
              " 'of': 13,\n",
              " 'do': 14,\n",
              " 'this': 15,\n",
              " 'me': 16,\n",
              " 'in': 17,\n",
              " 'have': 18,\n",
              " \"don't\": 19,\n",
              " 'was': 20,\n",
              " 'my': 21,\n",
              " 'are': 22,\n",
              " 'for': 23,\n",
              " 'your': 24,\n",
              " 'what': 25,\n",
              " \"i'm\": 26,\n",
              " 'we': 27,\n",
              " 'be': 28,\n",
              " 'she': 29,\n",
              " 'not': 30,\n",
              " 'want': 31,\n",
              " 'on': 32,\n",
              " 'with': 33,\n",
              " 'like': 34,\n",
              " 'know': 35,\n",
              " 'can': 36,\n",
              " 'his': 37,\n",
              " 'at': 38,\n",
              " 'all': 39,\n",
              " \"you're\": 40,\n",
              " 'how': 41,\n",
              " 'did': 42,\n",
              " 'him': 43,\n",
              " 'they': 44,\n",
              " 'think': 45,\n",
              " 'go': 46,\n",
              " 'and': 47,\n",
              " \"it's\": 48,\n",
              " \"can't\": 49,\n",
              " 'very': 50,\n",
              " 'time': 51,\n",
              " 'about': 52,\n",
              " 'here': 53,\n",
              " 'will': 54,\n",
              " 'get': 55,\n",
              " 'there': 56,\n",
              " 'her': 57,\n",
              " \"didn't\": 58,\n",
              " 'had': 59,\n",
              " 'as': 60,\n",
              " 'were': 61,\n",
              " 'if': 62,\n",
              " 'no': 63,\n",
              " 'one': 64,\n",
              " 'why': 65,\n",
              " 'just': 66,\n",
              " 'up': 67,\n",
              " 'has': 68,\n",
              " 'out': 69,\n",
              " 'going': 70,\n",
              " 'would': 71,\n",
              " 'good': 72,\n",
              " 'come': 73,\n",
              " 'so': 74,\n",
              " 'tell': 75,\n",
              " 'an': 76,\n",
              " 'when': 77,\n",
              " 'need': 78,\n",
              " \"i'll\": 79,\n",
              " 'by': 80,\n",
              " 'from': 81,\n",
              " 'see': 82,\n",
              " 'mary': 83,\n",
              " 'should': 84,\n",
              " 'really': 85,\n",
              " 'who': 86,\n",
              " \"that's\": 87,\n",
              " 'us': 88,\n",
              " 'help': 89,\n",
              " 'please': 90,\n",
              " 'been': 91,\n",
              " 'could': 92,\n",
              " 'take': 93,\n",
              " 'where': 94,\n",
              " 'never': 95,\n",
              " 'something': 96,\n",
              " 'now': 97,\n",
              " \"i've\": 98,\n",
              " 'more': 99,\n",
              " 'got': 100,\n",
              " 'too': 101,\n",
              " 'but': 102,\n",
              " 'than': 103,\n",
              " 'some': 104,\n",
              " 'make': 105,\n",
              " 'right': 106,\n",
              " 'much': 107,\n",
              " 'last': 108,\n",
              " 'work': 109,\n",
              " \"we're\": 110,\n",
              " 'am': 111,\n",
              " \"i'd\": 112,\n",
              " 'thought': 113,\n",
              " 'money': 114,\n",
              " 'anything': 115,\n",
              " 'say': 116,\n",
              " 'any': 117,\n",
              " \"he's\": 118,\n",
              " \"doesn't\": 119,\n",
              " 'told': 120,\n",
              " 'look': 121,\n",
              " 'sure': 122,\n",
              " 'back': 123,\n",
              " 'made': 124,\n",
              " 'day': 125,\n",
              " 'home': 126,\n",
              " 'car': 127,\n",
              " 'people': 128,\n",
              " 'give': 129,\n",
              " 'talk': 130,\n",
              " 'many': 131,\n",
              " 'love': 132,\n",
              " 'let': 133,\n",
              " 'way': 134,\n",
              " 'feel': 135,\n",
              " 'said': 136,\n",
              " 'our': 137,\n",
              " 'must': 138,\n",
              " 'house': 139,\n",
              " 'still': 140,\n",
              " 'lot': 141,\n",
              " 'went': 142,\n",
              " 'always': 143,\n",
              " 'happy': 144,\n",
              " 'believe': 145,\n",
              " 'again': 146,\n",
              " 'does': 147,\n",
              " \"let's\": 148,\n",
              " 'eat': 149,\n",
              " 'them': 150,\n",
              " 'only': 151,\n",
              " 'book': 152,\n",
              " 'better': 153,\n",
              " 'doing': 154,\n",
              " 'off': 155,\n",
              " 'new': 156,\n",
              " 'well': 157,\n",
              " 'before': 158,\n",
              " 'room': 159,\n",
              " 'long': 160,\n",
              " 'french': 161,\n",
              " 'little': 162,\n",
              " 'leave': 163,\n",
              " 'job': 164,\n",
              " 'old': 165,\n",
              " 'night': 166,\n",
              " 'school': 167,\n",
              " 'today': 168,\n",
              " 'down': 169,\n",
              " \"isn't\": 170,\n",
              " 'wanted': 171,\n",
              " 'two': 172,\n",
              " 'father': 173,\n",
              " 'may': 174,\n",
              " \"what's\": 175,\n",
              " 'everything': 176,\n",
              " \"won't\": 177,\n",
              " 'tomorrow': 178,\n",
              " 'try': 179,\n",
              " 'speak': 180,\n",
              " 'happened': 181,\n",
              " 'find': 182,\n",
              " 'yesterday': 183,\n",
              " 'every': 184,\n",
              " 'put': 185,\n",
              " 'three': 186,\n",
              " 'enough': 187,\n",
              " 'asked': 188,\n",
              " 'these': 189,\n",
              " 'left': 190,\n",
              " 'keep': 191,\n",
              " 'alone': 192,\n",
              " 'saw': 193,\n",
              " 'stay': 194,\n",
              " 'stop': 195,\n",
              " 'over': 196,\n",
              " 'ask': 197,\n",
              " 'understand': 198,\n",
              " 'ever': 199,\n",
              " 'next': 200,\n",
              " 'done': 201,\n",
              " 'or': 202,\n",
              " 'away': 203,\n",
              " 'into': 204,\n",
              " 'door': 205,\n",
              " \"they're\": 206,\n",
              " 'man': 207,\n",
              " \"you've\": 208,\n",
              " 'dog': 209,\n",
              " 'friends': 210,\n",
              " 'problem': 211,\n",
              " 'first': 212,\n",
              " 'their': 213,\n",
              " 'nothing': 214,\n",
              " 'sorry': 215,\n",
              " 'call': 216,\n",
              " 'hope': 217,\n",
              " \"tom's\": 218,\n",
              " 'gave': 219,\n",
              " 'things': 220,\n",
              " 'read': 221,\n",
              " 'after': 222,\n",
              " 'anyone': 223,\n",
              " 'everyone': 224,\n",
              " 'drink': 225,\n",
              " \"there's\": 226,\n",
              " 'late': 227,\n",
              " 'heard': 228,\n",
              " 'boston': 229,\n",
              " 'busy': 230,\n",
              " 'hear': 231,\n",
              " 'friend': 232,\n",
              " 'remember': 233,\n",
              " 'hard': 234,\n",
              " 'being': 235,\n",
              " 'wrong': 236,\n",
              " 'thing': 237,\n",
              " 'live': 238,\n",
              " 'buy': 239,\n",
              " 'without': 240,\n",
              " 'other': 241,\n",
              " 'took': 242,\n",
              " 'already': 243,\n",
              " 'idea': 244,\n",
              " 'looking': 245,\n",
              " \"you'd\": 246,\n",
              " 'life': 247,\n",
              " 'teacher': 248,\n",
              " 'seen': 249,\n",
              " \"haven't\": 250,\n",
              " 'english': 251,\n",
              " \"wasn't\": 252,\n",
              " 'came': 253,\n",
              " 'water': 254,\n",
              " 'knew': 255,\n",
              " 'mother': 256,\n",
              " 'even': 257,\n",
              " 'found': 258,\n",
              " 'lost': 259,\n",
              " 'used': 260,\n",
              " 'years': 261,\n",
              " \"couldn't\": 262,\n",
              " 'week': 263,\n",
              " 'children': 264,\n",
              " 'mind': 265,\n",
              " 'use': 266,\n",
              " 'big': 267,\n",
              " 'morning': 268,\n",
              " 'party': 269,\n",
              " 'soon': 270,\n",
              " 'ready': 271,\n",
              " 'married': 272,\n",
              " 'wait': 273,\n",
              " 'same': 274,\n",
              " 'kind': 275,\n",
              " 'play': 276,\n",
              " 'answer': 277,\n",
              " 'both': 278,\n",
              " \"you'll\": 279,\n",
              " 'someone': 280,\n",
              " 'tired': 281,\n",
              " \"aren't\": 282,\n",
              " 'name': 283,\n",
              " 'best': 284,\n",
              " 'yet': 285,\n",
              " 'bad': 286,\n",
              " 'beautiful': 287,\n",
              " 'show': 288,\n",
              " 'care': 289,\n",
              " 'happen': 290,\n",
              " 'around': 291,\n",
              " 'cold': 292,\n",
              " 'while': 293,\n",
              " \"she's\": 294,\n",
              " 'often': 295,\n",
              " 'open': 296,\n",
              " 'wants': 297,\n",
              " 'afraid': 298,\n",
              " 'great': 299,\n",
              " 'bed': 300,\n",
              " 'wish': 301,\n",
              " 'early': 302,\n",
              " 'getting': 303,\n",
              " 'looks': 304,\n",
              " 'talking': 305,\n",
              " 'else': 306,\n",
              " 'few': 307,\n",
              " 'true': 308,\n",
              " 'parents': 309,\n",
              " 'might': 310,\n",
              " 'such': 311,\n",
              " 'place': 312,\n",
              " \"we'll\": 313,\n",
              " 'coming': 314,\n",
              " 'nice': 315,\n",
              " 'glad': 316,\n",
              " 'yourself': 317,\n",
              " 'tried': 318,\n",
              " 'days': 319,\n",
              " 'myself': 320,\n",
              " 'turn': 321,\n",
              " 'which': 322,\n",
              " 'bought': 323,\n",
              " 'plan': 324,\n",
              " 'books': 325,\n",
              " 'year': 326,\n",
              " 'watch': 327,\n",
              " 'train': 328,\n",
              " 'almost': 329,\n",
              " 'hate': 330,\n",
              " \"wouldn't\": 331,\n",
              " 'letter': 332,\n",
              " 'those': 333,\n",
              " 'dinner': 334,\n",
              " 'knows': 335,\n",
              " 'hurt': 336,\n",
              " 'walk': 337,\n",
              " 'once': 338,\n",
              " 'sleep': 339,\n",
              " 'question': 340,\n",
              " 'since': 341,\n",
              " 'brother': 342,\n",
              " 'because': 343,\n",
              " 'fun': 344,\n",
              " 'anymore': 345,\n",
              " 'another': 346,\n",
              " 'sister': 347,\n",
              " 'write': 348,\n",
              " \"shouldn't\": 349,\n",
              " 'pay': 350,\n",
              " 'meet': 351,\n",
              " 'truth': 352,\n",
              " 'met': 353,\n",
              " 'meeting': 354,\n",
              " 'child': 355,\n",
              " 'pretty': 356,\n",
              " 'everybody': 357,\n",
              " 'felt': 358,\n",
              " 'family': 359,\n",
              " 'able': 360,\n",
              " 'waiting': 361,\n",
              " 'died': 362,\n",
              " 'world': 363,\n",
              " 'together': 364,\n",
              " 'food': 365,\n",
              " 'doctor': 366,\n",
              " 'hand': 367,\n",
              " 'nobody': 368,\n",
              " 'boy': 369,\n",
              " 'far': 370,\n",
              " 'most': 371,\n",
              " 'seems': 372,\n",
              " 'mine': 373,\n",
              " 'young': 374,\n",
              " 'matter': 375,\n",
              " 'trying': 376,\n",
              " 'own': 377,\n",
              " 'person': 378,\n",
              " 'forget': 379,\n",
              " 'surprised': 380,\n",
              " 'careful': 381,\n",
              " 'tonight': 382,\n",
              " 'quite': 383,\n",
              " 'thank': 384,\n",
              " 'started': 385,\n",
              " 'working': 386,\n",
              " 'wife': 387,\n",
              " 'change': 388,\n",
              " 'girl': 389,\n",
              " 'bus': 390,\n",
              " 'easy': 391,\n",
              " 'yours': 392,\n",
              " 'drive': 393,\n",
              " 'sick': 394,\n",
              " 'accident': 395,\n",
              " 'phone': 396,\n",
              " 'likes': 397,\n",
              " 'station': 398,\n",
              " 'word': 399,\n",
              " 'seem': 400,\n",
              " \"we've\": 401,\n",
              " 'coffee': 402,\n",
              " 'mean': 403,\n",
              " 'times': 404,\n",
              " 'anybody': 405,\n",
              " 'study': 406,\n",
              " 'picture': 407,\n",
              " 'important': 408,\n",
              " 'cat': 409,\n",
              " 'looked': 410,\n",
              " 'story': 411,\n",
              " 'music': 412,\n",
              " 'movie': 413,\n",
              " 'lunch': 414,\n",
              " 'mistake': 415,\n",
              " 'broke': 416,\n",
              " 'rain': 417,\n",
              " 'gone': 418,\n",
              " 'finished': 419,\n",
              " 'each': 420,\n",
              " 'start': 421,\n",
              " 'longer': 422,\n",
              " 'hours': 423,\n",
              " 'reading': 424,\n",
              " 'small': 425,\n",
              " 'makes': 426,\n",
              " 'learn': 427,\n",
              " 'probably': 428,\n",
              " 'stupid': 429,\n",
              " 'wonder': 430,\n",
              " 'tv': 431,\n",
              " 'difficult': 432,\n",
              " 'says': 433,\n",
              " 'japanese': 434,\n",
              " 'against': 435,\n",
              " 'ten': 436,\n",
              " 'lives': 437,\n",
              " 'supposed': 438,\n",
              " 'turned': 439,\n",
              " 'sing': 440,\n",
              " 'himself': 441,\n",
              " 'trust': 442,\n",
              " 'japan': 443,\n",
              " 'town': 444,\n",
              " 'angry': 445,\n",
              " 'questions': 446,\n",
              " 'interesting': 447,\n",
              " 'bit': 448,\n",
              " 'eyes': 449,\n",
              " 'window': 450,\n",
              " 'stand': 451,\n",
              " 'students': 452,\n",
              " 'close': 453,\n",
              " 'police': 454,\n",
              " 'number': 455,\n",
              " 'until': 456,\n",
              " 'under': 457,\n",
              " 'bicycle': 458,\n",
              " 'having': 459,\n",
              " 'playing': 460,\n",
              " 'advice': 461,\n",
              " 'run': 462,\n",
              " 'free': 463,\n",
              " 'arrived': 464,\n",
              " 'afternoon': 465,\n",
              " 'sit': 466,\n",
              " 'game': 467,\n",
              " 'appreciate': 468,\n",
              " 'office': 469,\n",
              " 'listen': 470,\n",
              " 'fell': 471,\n",
              " 'thinking': 472,\n",
              " 'hot': 473,\n",
              " 'exactly': 474,\n",
              " 'trouble': 475,\n",
              " 'table': 476,\n",
              " 'agree': 477,\n",
              " 'business': 478,\n",
              " 'ago': 479,\n",
              " 'win': 480,\n",
              " 'hurry': 481,\n",
              " 'wine': 482,\n",
              " 'quit': 483,\n",
              " 'guess': 484,\n",
              " 'worry': 485,\n",
              " 'finish': 486,\n",
              " 'taking': 487,\n",
              " 'country': 488,\n",
              " 'fast': 489,\n",
              " 'dress': 490,\n",
              " 'ran': 491,\n",
              " 'weather': 492,\n",
              " 'baby': 493,\n",
              " 'decided': 494,\n",
              " 'age': 495,\n",
              " 'outside': 496,\n",
              " 'making': 497,\n",
              " 'proud': 498,\n",
              " 'five': 499,\n",
              " 'hair': 500,\n",
              " 'eating': 501,\n",
              " 'red': 502,\n",
              " 'moment': 503,\n",
              " 'part': 504,\n",
              " 'hungry': 505,\n",
              " 'high': 506,\n",
              " 'miss': 507,\n",
              " 'fire': 508,\n",
              " 'works': 509,\n",
              " 'son': 510,\n",
              " 'lie': 511,\n",
              " 'city': 512,\n",
              " 'breakfast': 513,\n",
              " 'spend': 514,\n",
              " 'swim': 515,\n",
              " 'cut': 516,\n",
              " 'cake': 517,\n",
              " 'light': 518,\n",
              " \"should've\": 519,\n",
              " 'homework': 520,\n",
              " 'kept': 521,\n",
              " 'hotel': 522,\n",
              " 'advised': 523,\n",
              " 'reason': 524,\n",
              " 'caught': 525,\n",
              " 'hands': 526,\n",
              " 'summer': 527,\n",
              " 'born': 528,\n",
              " 'catch': 529,\n",
              " 'shoes': 530,\n",
              " 'usually': 531,\n",
              " 'girlfriend': 532,\n",
              " 'forgot': 533,\n",
              " 'interested': 534,\n",
              " 'become': 535,\n",
              " 'bring': 536,\n",
              " 'trip': 537,\n",
              " 'different': 538,\n",
              " 'promise': 539,\n",
              " 'eaten': 540,\n",
              " 'end': 541,\n",
              " 'funny': 542,\n",
              " 'possible': 543,\n",
              " 'box': 544,\n",
              " 'rich': 545,\n",
              " 'clothes': 546,\n",
              " 'favorite': 547,\n",
              " 'dead': 548,\n",
              " 'song': 549,\n",
              " 'face': 550,\n",
              " 'near': 551,\n",
              " 'saying': 552,\n",
              " 'die': 553,\n",
              " 'month': 554,\n",
              " 'safe': 555,\n",
              " 'later': 556,\n",
              " 'maybe': 557,\n",
              " 'quickly': 558,\n",
              " 'through': 559,\n",
              " 'expensive': 560,\n",
              " 'its': 561,\n",
              " \"who's\": 562,\n",
              " 'paid': 563,\n",
              " 'serious': 564,\n",
              " 'guy': 565,\n",
              " 'class': 566,\n",
              " 'whether': 567,\n",
              " 'comes': 568,\n",
              " 'head': 569,\n",
              " 'became': 570,\n",
              " 'team': 571,\n",
              " \"weren't\": 572,\n",
              " 'lose': 573,\n",
              " 'needs': 574,\n",
              " 'cannot': 575,\n",
              " 'along': 576,\n",
              " 'watching': 577,\n",
              " 'rather': 578,\n",
              " \"mary's\": 579,\n",
              " 'hat': 580,\n",
              " 'paper': 581,\n",
              " 'telling': 582,\n",
              " 'chance': 583,\n",
              " 'called': 584,\n",
              " 'tree': 585,\n",
              " 'visit': 586,\n",
              " 'invited': 587,\n",
              " 'computer': 588,\n",
              " 'explain': 589,\n",
              " 'park': 590,\n",
              " 'hit': 591,\n",
              " 'ok': 592,\n",
              " 'point': 593,\n",
              " 'killed': 594,\n",
              " 'stayed': 595,\n",
              " 'enjoy': 596,\n",
              " 'spent': 597,\n",
              " 'kids': 598,\n",
              " 'mad': 599,\n",
              " 'smoking': 600,\n",
              " 'minutes': 601,\n",
              " 'figured': 602,\n",
              " 'lucky': 603,\n",
              " 'others': 604,\n",
              " 'then': 605,\n",
              " 'sometimes': 606,\n",
              " 'sound': 607,\n",
              " 'key': 608,\n",
              " 'clean': 609,\n",
              " 'seeing': 610,\n",
              " 'death': 611,\n",
              " 'expected': 612,\n",
              " 'large': 613,\n",
              " 'secret': 614,\n",
              " 'cost': 615,\n",
              " 'real': 616,\n",
              " 'snow': 617,\n",
              " 'words': 618,\n",
              " 'drinking': 619,\n",
              " 'famous': 620,\n",
              " 'strange': 621,\n",
              " 'worth': 622,\n",
              " 'living': 623,\n",
              " 'asleep': 624,\n",
              " 'older': 625,\n",
              " 'hospital': 626,\n",
              " 'language': 627,\n",
              " 'present': 628,\n",
              " 'hour': 629,\n",
              " 'feeling': 630,\n",
              " 'front': 631,\n",
              " 'tennis': 632,\n",
              " 'whatever': 633,\n",
              " 'air': 634,\n",
              " 'rest': 635,\n",
              " 'hold': 636,\n",
              " 'case': 637,\n",
              " 'short': 638,\n",
              " 'gets': 639,\n",
              " 'choice': 640,\n",
              " 'report': 641,\n",
              " 'tea': 642,\n",
              " 'second': 643,\n",
              " 'woman': 644,\n",
              " 'worried': 645,\n",
              " 'beer': 646,\n",
              " 'running': 647,\n",
              " 'crazy': 648,\n",
              " 'check': 649,\n",
              " 'sat': 650,\n",
              " 'quiet': 651,\n",
              " 'stopped': 652,\n",
              " 'river': 653,\n",
              " 'break': 654,\n",
              " 'street': 655,\n",
              " 'necessary': 656,\n",
              " 'birthday': 657,\n",
              " 'daughter': 658,\n",
              " 'full': 659,\n",
              " 'weekend': 660,\n",
              " 'dark': 661,\n",
              " 'decision': 662,\n",
              " \"where's\": 663,\n",
              " 'handle': 664,\n",
              " 'somebody': 665,\n",
              " 'prefer': 666,\n",
              " 'whole': 667,\n",
              " 'evening': 668,\n",
              " 'takes': 669,\n",
              " 'student': 670,\n",
              " 'wearing': 671,\n",
              " 'ate': 672,\n",
              " 'fish': 673,\n",
              " 'behind': 674,\n",
              " 'building': 675,\n",
              " 'happens': 676,\n",
              " 'wrote': 677,\n",
              " 'restaurant': 678,\n",
              " 'news': 679,\n",
              " 'between': 680,\n",
              " 'choose': 681,\n",
              " 'helped': 682,\n",
              " 'shut': 683,\n",
              " 'needed': 684,\n",
              " 'learned': 685,\n",
              " 'alive': 686,\n",
              " 'actually': 687,\n",
              " 'drunk': 688,\n",
              " 'war': 689,\n",
              " 'loves': 690,\n",
              " 'coat': 691,\n",
              " 'changed': 692,\n",
              " 'walked': 693,\n",
              " 'deal': 694,\n",
              " 'glass': 695,\n",
              " 'perfect': 696,\n",
              " 'fine': 697,\n",
              " 'goes': 698,\n",
              " 'mistakes': 699,\n",
              " 'boss': 700,\n",
              " 'heart': 701,\n",
              " 'sense': 702,\n",
              " 'liked': 703,\n",
              " 'upset': 704,\n",
              " 'happening': 705,\n",
              " 'began': 706,\n",
              " 'missed': 707,\n",
              " 'thirty': 708,\n",
              " 'noise': 709,\n",
              " 'kill': 710,\n",
              " 'piano': 711,\n",
              " 'written': 712,\n",
              " 'company': 713,\n",
              " 'touch': 714,\n",
              " 'broken': 715,\n",
              " 'attention': 716,\n",
              " 'least': 717,\n",
              " 'known': 718,\n",
              " 'luck': 719,\n",
              " 'expect': 720,\n",
              " 'forward': 721,\n",
              " 'list': 722,\n",
              " 'completely': 723,\n",
              " 'crying': 724,\n",
              " 'scared': 725,\n",
              " 'lived': 726,\n",
              " 'wear': 727,\n",
              " 'studying': 728,\n",
              " 'minute': 729,\n",
              " 'move': 730,\n",
              " 'floor': 731,\n",
              " 'save': 732,\n",
              " 'six': 733,\n",
              " 'desk': 734,\n",
              " 'offer': 735,\n",
              " 'allowed': 736,\n",
              " 'dangerous': 737,\n",
              " 'thanks': 738,\n",
              " 'store': 739,\n",
              " \"it'll\": 740,\n",
              " 'willing': 741,\n",
              " 'dogs': 742,\n",
              " 'cup': 743,\n",
              " 'dance': 744,\n",
              " 'future': 745,\n",
              " 'dictionary': 746,\n",
              " 'join': 747,\n",
              " 'monday': 748,\n",
              " 'leaving': 749,\n",
              " 'girls': 750,\n",
              " 'brought': 751,\n",
              " 'vacation': 752,\n",
              " 'situation': 753,\n",
              " 'library': 754,\n",
              " 'bag': 755,\n",
              " 'college': 756,\n",
              " 'dream': 757,\n",
              " 'pain': 758,\n",
              " 'kiss': 759,\n",
              " 'side': 760,\n",
              " 'weight': 761,\n",
              " 'cook': 762,\n",
              " \"o'clock\": 763,\n",
              " 'plane': 764,\n",
              " 'begin': 765,\n",
              " 'speaks': 766,\n",
              " 'closed': 767,\n",
              " 'abroad': 768,\n",
              " 'tie': 769,\n",
              " 'joke': 770,\n",
              " 'opened': 771,\n",
              " 'umbrella': 772,\n",
              " 'either': 773,\n",
              " 'worked': 774,\n",
              " 'sunday': 775,\n",
              " 'ice': 776,\n",
              " 'health': 777,\n",
              " 'slept': 778,\n",
              " 'suppose': 779,\n",
              " \"he'll\": 780,\n",
              " 'immediately': 781,\n",
              " 'dollars': 782,\n",
              " 'talked': 783,\n",
              " 'date': 784,\n",
              " 'piece': 785,\n",
              " 'road': 786,\n",
              " 'half': 787,\n",
              " 'apologize': 788,\n",
              " 'beach': 789,\n",
              " 'white': 790,\n",
              " 'loved': 791,\n",
              " 'finally': 792,\n",
              " 'order': 793,\n",
              " 'tall': 794,\n",
              " 'danger': 795,\n",
              " 'garden': 796,\n",
              " 'disappointed': 797,\n",
              " 'ship': 798,\n",
              " 'men': 799,\n",
              " 'uncle': 800,\n",
              " 'somewhere': 801,\n",
              " 'writing': 802,\n",
              " 'american': 803,\n",
              " 'sent': 804,\n",
              " 'thinks': 805,\n",
              " 'prepared': 806,\n",
              " 'strong': 807,\n",
              " 'ride': 808,\n",
              " 'seat': 809,\n",
              " 'laugh': 810,\n",
              " 'involved': 811,\n",
              " 'discuss': 812,\n",
              " 'christmas': 813,\n",
              " 'dressed': 814,\n",
              " 'storm': 815,\n",
              " 'problems': 816,\n",
              " 'boys': 817,\n",
              " 'won': 818,\n",
              " 'accept': 819,\n",
              " 'set': 820,\n",
              " 'arrive': 821,\n",
              " 'cry': 822,\n",
              " 'during': 823,\n",
              " 'novel': 824,\n",
              " 'smoke': 825,\n",
              " 'blue': 826,\n",
              " 'tokyo': 827,\n",
              " 'waste': 828,\n",
              " 'speaking': 829,\n",
              " 'owe': 830,\n",
              " 'staying': 831,\n",
              " 'raining': 832,\n",
              " \"hasn't\": 833,\n",
              " 'poor': 834,\n",
              " 'shot': 835,\n",
              " 'mountain': 836,\n",
              " 'rules': 837,\n",
              " 'figure': 838,\n",
              " 'wake': 839,\n",
              " 'sleeping': 840,\n",
              " 'smart': 841,\n",
              " 'fix': 842,\n",
              " '30': 843,\n",
              " 'carefully': 844,\n",
              " 'gun': 845,\n",
              " 'across': 846,\n",
              " 'cute': 847,\n",
              " 'promised': 848,\n",
              " 'certain': 849,\n",
              " 'guys': 850,\n",
              " 'sitting': 851,\n",
              " 'concert': 852,\n",
              " 'voice': 853,\n",
              " 'honest': 854,\n",
              " 'price': 855,\n",
              " 'whose': 856,\n",
              " 'radio': 857,\n",
              " 'couple': 858,\n",
              " 'grow': 859,\n",
              " 'easily': 860,\n",
              " 'arm': 861,\n",
              " 'driving': 862,\n",
              " 'camera': 863,\n",
              " 'opinion': 864,\n",
              " 'lying': 865,\n",
              " 'impressed': 866,\n",
              " 'given': 867,\n",
              " 'difference': 868,\n",
              " 'wonderful': 869,\n",
              " 'impossible': 870,\n",
              " 'regret': 871,\n",
              " 'cats': 872,\n",
              " 'several': 873,\n",
              " 'lake': 874,\n",
              " 'foreign': 875,\n",
              " 'sounds': 876,\n",
              " 'medicine': 877,\n",
              " 'feelings': 878,\n",
              " 'television': 879,\n",
              " \"how's\": 880,\n",
              " 'ticket': 881,\n",
              " 'less': 882,\n",
              " 'top': 883,\n",
              " 'heavy': 884,\n",
              " 'showed': 885,\n",
              " 'horse': 886,\n",
              " 'america': 887,\n",
              " 'black': 888,\n",
              " 'knife': 889,\n",
              " 'succeed': 890,\n",
              " 'lawyer': 891,\n",
              " 'speech': 892,\n",
              " 'earlier': 893,\n",
              " 'milk': 894,\n",
              " 'foot': 895,\n",
              " 'success': 896,\n",
              " 'kid': 897,\n",
              " 'simple': 898,\n",
              " 'bank': 899,\n",
              " 'travel': 900,\n",
              " 'none': 901,\n",
              " 'kissed': 902,\n",
              " 'listening': 903,\n",
              " 'solve': 904,\n",
              " 'means': 905,\n",
              " 'keys': 906,\n",
              " 'built': 907,\n",
              " 'bird': 908,\n",
              " 'test': 909,\n",
              " 'nervous': 910,\n",
              " 'telephone': 911,\n",
              " 'guitar': 912,\n",
              " 'winter': 913,\n",
              " 'special': 914,\n",
              " 'taken': 915,\n",
              " 'terrible': 916,\n",
              " 'line': 917,\n",
              " 'count': 918,\n",
              " 'sad': 919,\n",
              " 'borrow': 920,\n",
              " 'likely': 921,\n",
              " 'favor': 922,\n",
              " 'ideas': 923,\n",
              " 'asking': 924,\n",
              " 'boyfriend': 925,\n",
              " 'laughed': 926,\n",
              " 'hardly': 927,\n",
              " 'bill': 928,\n",
              " \"they'll\": 929,\n",
              " 'learning': 930,\n",
              " 'machine': 931,\n",
              " 'map': 932,\n",
              " 'seriously': 933,\n",
              " 'address': 934,\n",
              " 'fired': 935,\n",
              " 'shop': 936,\n",
              " 'painted': 937,\n",
              " 'apple': 938,\n",
              " 'blame': 939,\n",
              " 'project': 940,\n",
              " 'convinced': 941,\n",
              " 'satisfied': 942,\n",
              " 'swimming': 943,\n",
              " 'carry': 944,\n",
              " '2': 945,\n",
              " 'women': 946,\n",
              " 'shopping': 947,\n",
              " 'anywhere': 948,\n",
              " 'chair': 949,\n",
              " 'ahead': 950,\n",
              " 'hid': 951,\n",
              " 'recognize': 952,\n",
              " 'bath': 953,\n",
              " 'herself': 954,\n",
              " 'decide': 955,\n",
              " 'london': 956,\n",
              " 'enjoyed': 957,\n",
              " 'surprise': 958,\n",
              " 'fight': 959,\n",
              " 'failed': 960,\n",
              " 'trees': 961,\n",
              " 'shirt': 962,\n",
              " 'eye': 963,\n",
              " 'forced': 964,\n",
              " 'clear': 965,\n",
              " 'fault': 966,\n",
              " 'charge': 967,\n",
              " 'plans': 968,\n",
              " 'follow': 969,\n",
              " 'correct': 970,\n",
              " 'flowers': 971,\n",
              " 'refused': 972,\n",
              " 'information': 973,\n",
              " 'stuff': 974,\n",
              " 'pass': 975,\n",
              " 'traffic': 976,\n",
              " 'worse': 977,\n",
              " 'animals': 978,\n",
              " 'instead': 979,\n",
              " 'studied': 980,\n",
              " 'prison': 981,\n",
              " 'missing': 982,\n",
              " 'fall': 983,\n",
              " \"we'd\": 984,\n",
              " 'movies': 985,\n",
              " 'enemy': 986,\n",
              " 'wall': 987,\n",
              " 'past': 988,\n",
              " 'message': 989,\n",
              " \"one's\": 990,\n",
              " 'teach': 991,\n",
              " 'injured': 992,\n",
              " 'newspaper': 993,\n",
              " 'held': 994,\n",
              " 'excited': 995,\n",
              " 'memory': 996,\n",
              " 'lock': 997,\n",
              " 'ashamed': 998,\n",
              " 'twice': 999,\n",
              " 'inside': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIwlGhNDykzn"
      },
      "source": [
        "doc[\"fr_indices\"] = tokenizer_fr.texts_to_sequences(doc.iloc[:,1])\n",
        "doc[\"en_indices\"] = tokenizer_en.texts_to_sequences(doc.iloc[:,0])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUjLaIjB0e-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "71e2b836-237a-4f81-eb8d-a0aa26db54d2"
      },
      "source": [
        "doc.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>fr_indices</th>\n",
              "      <th>en_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18344</th>\n",
              "      <td>&lt;start&gt; I want to be safe. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Je veux être en sécurité. &lt;end&gt;</td>\n",
              "      <td>[1, 3, 37, 50, 20, 616, 2]</td>\n",
              "      <td>[1, 3, 31, 5, 28, 555, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48108</th>\n",
              "      <td>&lt;start&gt; Where are your manners? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Qu'avez-vous fait de vos bonnes manièr...</td>\n",
              "      <td>[1, 872, 6, 42, 4, 200, 1098, 2021, 2]</td>\n",
              "      <td>[1, 94, 22, 24, 1889, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24273</th>\n",
              "      <td>&lt;start&gt; The glass is empty. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Le verre est vide. &lt;end&gt;</td>\n",
              "      <td>[1, 11, 432, 15, 1125, 2]</td>\n",
              "      <td>[1, 6, 695, 8, 1003, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104113</th>\n",
              "      <td>&lt;start&gt; I agree with what you've written. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Je suis d'accord avec ce que tu as écr...</td>\n",
              "      <td>[1, 3, 25, 373, 39, 14, 7, 13, 67, 427, 2]</td>\n",
              "      <td>[1, 3, 477, 33, 25, 208, 712, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50387</th>\n",
              "      <td>&lt;start&gt; I don't miss you at all. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; Vous ne me manquez pas du tout. &lt;end&gt;</td>\n",
              "      <td>[1, 6, 9, 24, 2709, 5, 40, 34, 2]</td>\n",
              "      <td>[1, 3, 19, 507, 4, 38, 39, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0  ...                        en_indices\n",
              "18344                  <start> I want to be safe. <end>  ...         [1, 3, 31, 5, 28, 555, 2]\n",
              "48108             <start> Where are your manners? <end>  ...          [1, 94, 22, 24, 1889, 2]\n",
              "24273                 <start> The glass is empty. <end>  ...           [1, 6, 695, 8, 1003, 2]\n",
              "104113  <start> I agree with what you've written. <end>  ...  [1, 3, 477, 33, 25, 208, 712, 2]\n",
              "50387            <start> I don't miss you at all. <end>  ...     [1, 3, 19, 507, 4, 38, 39, 2]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6miVbPY0xZw"
      },
      "source": [
        "# Use of Keras to create token sequences of the same length\n",
        "padded_fr_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"fr_indices\"], padding=\"post\")\n",
        "padded_en_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"en_indices\"], padding=\"post\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy_ou60_3X4X"
      },
      "source": [
        "# Creation of tf.data.Dataset for each of the French and English tensors\n",
        "fr_ds = tf.data.Dataset.from_tensor_slices(padded_fr_indices)\n",
        "en_ds = tf.data.Dataset.from_tensor_slices(padded_en_indices)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFIfUb1i7ia3"
      },
      "source": [
        "# Create a tensorflow dataset complet\n",
        "tf_ds = tf.data.Dataset.zip((fr_ds, en_ds))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueIigsED1nWq"
      },
      "source": [
        "# Creation of variables that we will reuse for our models\n",
        "BATCH_SIZE = 64\n",
        "TAKE_SIZE = int(0.7*len(doc)/BATCH_SIZE)\n",
        "BUFFER_SIZE = TAKE_SIZE * BATCH_SIZE\n",
        "steps_per_epoch = TAKE_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(tokenizer_fr.word_index)\n",
        "vocab_tar_size = len(tokenizer_en.word_index)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0aIpW5cZmnh"
      },
      "source": [
        "# Shuffle & Batch\n",
        "tf_ds = tf_ds.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCrDIAkMZPUM"
      },
      "source": [
        "# Train Test Split\n",
        "train_data = tf_ds.take(TAKE_SIZE).shuffle(TAKE_SIZE)\n",
        "test_data = tf_ds.skip(TAKE_SIZE).shuffle(BUFFER_SIZE-TAKE_SIZE)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EmEescP6jSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5ab86a-926c-44a5-b823-4a2a0a415c1b"
      },
      "source": [
        "input_text, output_text = next(iter(train_data))\n",
        "print(input_text.numpy().shape)\n",
        "print(output_text.numpy().shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 49)\n",
            "(64, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2PAxudOG9GG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47fd739-f00c-4e40-fa02-9d66ebbd2494"
      },
      "source": [
        "vocab_inp_size"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17747"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbbOi7sYVDU2"
      },
      "source": [
        "# Encode\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cmD6jUP74Rx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb162b3-09b1-4990-9b70-258da571e755"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size +1, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# Sample output\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(input_text, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 49, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq2nP5l_76LS"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # This is done to calculate our \"attention\" score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # We get 1 on the last axis because we apply the score to self.V\n",
        "    # The shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KetnxQvd8bF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85755d7b-c2f4-4a88-bfde-4af7f60c2363"
      },
      "source": [
        "attention_layer = BahdanauAttention(100)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 49, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruo6sIk98eLs"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenate == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # Passing from the concatenated vector to the GRU layer\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPW5WmrY8hyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76d7ac5-7be4-4056-939d-c8119aca1854"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size + 1, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 9493)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2h8jDTT8wHq"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ldPbErh8j1x"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBFavAT78w25"
      },
      "source": [
        "import os\n",
        "checkpoint_dir = './training_checkpoints2'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8V4m-De84om"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUAgTA4-8zsX"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims(targ[:,0], 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgoAc_Sd85tt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ece2ac3-4036-4531-8969-83dbf6c8058a"
      },
      "source": [
        "import time\n",
        "EPOCHS = 10\n",
        "steps_per_epoch = TAKE_SIZE\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_data.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  \n",
        "  # saving (checkpoint) the model every epoch\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.6768\n",
            "Epoch 1 Batch 10 Loss 1.0945\n",
            "Epoch 1 Batch 20 Loss 1.1084\n",
            "Epoch 1 Batch 30 Loss 1.1516\n",
            "Epoch 1 Batch 40 Loss 1.0215\n",
            "Epoch 1 Batch 50 Loss 1.1138\n",
            "Epoch 1 Batch 60 Loss 1.0599\n",
            "Epoch 1 Batch 70 Loss 1.1098\n",
            "Epoch 1 Batch 80 Loss 1.0791\n",
            "Epoch 1 Batch 90 Loss 1.0371\n",
            "Epoch 1 Batch 100 Loss 1.0077\n",
            "Epoch 1 Batch 110 Loss 0.9639\n",
            "Epoch 1 Batch 120 Loss 1.0445\n",
            "Epoch 1 Batch 130 Loss 1.0129\n",
            "Epoch 1 Batch 140 Loss 1.0308\n",
            "Epoch 1 Batch 150 Loss 0.9066\n",
            "Epoch 1 Batch 160 Loss 1.0090\n",
            "Epoch 1 Batch 170 Loss 0.9995\n",
            "Epoch 1 Batch 180 Loss 0.9122\n",
            "Epoch 1 Batch 190 Loss 0.9500\n",
            "Epoch 1 Batch 200 Loss 0.9307\n",
            "Epoch 1 Batch 210 Loss 0.8807\n",
            "Epoch 1 Batch 220 Loss 0.9226\n",
            "Epoch 1 Batch 230 Loss 0.9380\n",
            "Epoch 1 Batch 240 Loss 0.7882\n",
            "Epoch 1 Batch 250 Loss 0.9326\n",
            "Epoch 1 Batch 260 Loss 0.8901\n",
            "Epoch 1 Batch 270 Loss 0.9136\n",
            "Epoch 1 Batch 280 Loss 0.8741\n",
            "Epoch 1 Batch 290 Loss 0.8974\n",
            "Epoch 1 Batch 300 Loss 0.8045\n",
            "Epoch 1 Batch 310 Loss 0.9830\n",
            "Epoch 1 Batch 320 Loss 0.8048\n",
            "Epoch 1 Batch 330 Loss 0.8192\n",
            "Epoch 1 Batch 340 Loss 0.7602\n",
            "Epoch 1 Batch 350 Loss 0.7372\n",
            "Epoch 1 Batch 360 Loss 0.8544\n",
            "Epoch 1 Batch 370 Loss 0.8923\n",
            "Epoch 1 Batch 380 Loss 0.7399\n",
            "Epoch 1 Batch 390 Loss 0.7517\n",
            "Epoch 1 Batch 400 Loss 0.8951\n",
            "Epoch 1 Batch 410 Loss 0.8916\n",
            "Epoch 1 Batch 420 Loss 0.8605\n",
            "Epoch 1 Batch 430 Loss 0.7371\n",
            "Epoch 1 Batch 440 Loss 0.8502\n",
            "Epoch 1 Batch 450 Loss 0.8190\n",
            "Epoch 1 Batch 460 Loss 0.8173\n",
            "Epoch 1 Batch 470 Loss 0.7945\n",
            "Epoch 1 Batch 480 Loss 0.7504\n",
            "Epoch 1 Batch 490 Loss 0.8493\n",
            "Epoch 1 Batch 500 Loss 0.8107\n",
            "Epoch 1 Batch 510 Loss 0.7354\n",
            "Epoch 1 Batch 520 Loss 0.7763\n",
            "Epoch 1 Batch 530 Loss 0.8124\n",
            "Epoch 1 Batch 540 Loss 0.8545\n",
            "Epoch 1 Loss 0.9085\n",
            "Time taken for 1 epoch 203.22159361839294 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.7326\n",
            "Epoch 2 Batch 10 Loss 0.6380\n",
            "Epoch 2 Batch 20 Loss 0.6626\n",
            "Epoch 2 Batch 30 Loss 0.8149\n",
            "Epoch 2 Batch 40 Loss 0.7629\n",
            "Epoch 2 Batch 50 Loss 0.7667\n",
            "Epoch 2 Batch 60 Loss 0.6967\n",
            "Epoch 2 Batch 70 Loss 0.8163\n",
            "Epoch 2 Batch 80 Loss 0.7702\n",
            "Epoch 2 Batch 90 Loss 0.7419\n",
            "Epoch 2 Batch 100 Loss 0.7219\n",
            "Epoch 2 Batch 110 Loss 0.7086\n",
            "Epoch 2 Batch 120 Loss 0.6446\n",
            "Epoch 2 Batch 130 Loss 0.7510\n",
            "Epoch 2 Batch 140 Loss 0.8218\n",
            "Epoch 2 Batch 150 Loss 0.6901\n",
            "Epoch 2 Batch 160 Loss 0.7335\n",
            "Epoch 2 Batch 170 Loss 0.7345\n",
            "Epoch 2 Batch 180 Loss 0.6441\n",
            "Epoch 2 Batch 190 Loss 0.6936\n",
            "Epoch 2 Batch 200 Loss 0.7000\n",
            "Epoch 2 Batch 210 Loss 0.6423\n",
            "Epoch 2 Batch 220 Loss 0.6780\n",
            "Epoch 2 Batch 230 Loss 0.6969\n",
            "Epoch 2 Batch 240 Loss 0.6661\n",
            "Epoch 2 Batch 250 Loss 0.6790\n",
            "Epoch 2 Batch 260 Loss 0.7689\n",
            "Epoch 2 Batch 270 Loss 0.7700\n",
            "Epoch 2 Batch 280 Loss 0.7519\n",
            "Epoch 2 Batch 290 Loss 0.6746\n",
            "Epoch 2 Batch 300 Loss 0.6763\n",
            "Epoch 2 Batch 310 Loss 0.6091\n",
            "Epoch 2 Batch 320 Loss 0.6581\n",
            "Epoch 2 Batch 330 Loss 0.6196\n",
            "Epoch 2 Batch 340 Loss 0.6544\n",
            "Epoch 2 Batch 350 Loss 0.6303\n",
            "Epoch 2 Batch 360 Loss 0.6354\n",
            "Epoch 2 Batch 370 Loss 0.6193\n",
            "Epoch 2 Batch 380 Loss 0.6761\n",
            "Epoch 2 Batch 390 Loss 0.6926\n",
            "Epoch 2 Batch 400 Loss 0.6084\n",
            "Epoch 2 Batch 410 Loss 0.6267\n",
            "Epoch 2 Batch 420 Loss 0.6251\n",
            "Epoch 2 Batch 430 Loss 0.5721\n",
            "Epoch 2 Batch 440 Loss 0.7387\n",
            "Epoch 2 Batch 450 Loss 0.6985\n",
            "Epoch 2 Batch 460 Loss 0.6913\n",
            "Epoch 2 Batch 470 Loss 0.6908\n",
            "Epoch 2 Batch 480 Loss 0.5866\n",
            "Epoch 2 Batch 490 Loss 0.6458\n",
            "Epoch 2 Batch 500 Loss 0.5867\n",
            "Epoch 2 Batch 510 Loss 0.6279\n",
            "Epoch 2 Batch 520 Loss 0.6027\n",
            "Epoch 2 Batch 530 Loss 0.5787\n",
            "Epoch 2 Batch 540 Loss 0.6880\n",
            "Epoch 2 Loss 0.6833\n",
            "Time taken for 1 epoch 163.06687951087952 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.5660\n",
            "Epoch 3 Batch 10 Loss 0.5156\n",
            "Epoch 3 Batch 20 Loss 0.6475\n",
            "Epoch 3 Batch 30 Loss 0.5763\n",
            "Epoch 3 Batch 40 Loss 0.4454\n",
            "Epoch 3 Batch 50 Loss 0.6320\n",
            "Epoch 3 Batch 60 Loss 0.5304\n",
            "Epoch 3 Batch 70 Loss 0.5537\n",
            "Epoch 3 Batch 80 Loss 0.5962\n",
            "Epoch 3 Batch 90 Loss 0.4795\n",
            "Epoch 3 Batch 100 Loss 0.5775\n",
            "Epoch 3 Batch 110 Loss 0.6327\n",
            "Epoch 3 Batch 120 Loss 0.5825\n",
            "Epoch 3 Batch 130 Loss 0.5424\n",
            "Epoch 3 Batch 140 Loss 0.6175\n",
            "Epoch 3 Batch 150 Loss 0.6198\n",
            "Epoch 3 Batch 160 Loss 0.4434\n",
            "Epoch 3 Batch 170 Loss 0.5424\n",
            "Epoch 3 Batch 180 Loss 0.5057\n",
            "Epoch 3 Batch 190 Loss 0.4368\n",
            "Epoch 3 Batch 200 Loss 0.5691\n",
            "Epoch 3 Batch 210 Loss 0.4854\n",
            "Epoch 3 Batch 220 Loss 0.5636\n",
            "Epoch 3 Batch 230 Loss 0.6126\n",
            "Epoch 3 Batch 240 Loss 0.4922\n",
            "Epoch 3 Batch 250 Loss 0.5287\n",
            "Epoch 3 Batch 260 Loss 0.5197\n",
            "Epoch 3 Batch 270 Loss 0.5783\n",
            "Epoch 3 Batch 280 Loss 0.5271\n",
            "Epoch 3 Batch 290 Loss 0.5449\n",
            "Epoch 3 Batch 300 Loss 0.5206\n",
            "Epoch 3 Batch 310 Loss 0.5315\n",
            "Epoch 3 Batch 320 Loss 0.6176\n",
            "Epoch 3 Batch 330 Loss 0.5147\n",
            "Epoch 3 Batch 340 Loss 0.5415\n",
            "Epoch 3 Batch 350 Loss 0.4777\n",
            "Epoch 3 Batch 360 Loss 0.5360\n",
            "Epoch 3 Batch 370 Loss 0.5137\n",
            "Epoch 3 Batch 380 Loss 0.4950\n",
            "Epoch 3 Batch 390 Loss 0.5298\n",
            "Epoch 3 Batch 400 Loss 0.4712\n",
            "Epoch 3 Batch 410 Loss 0.4670\n",
            "Epoch 3 Batch 420 Loss 0.4743\n",
            "Epoch 3 Batch 430 Loss 0.5182\n",
            "Epoch 3 Batch 440 Loss 0.5636\n",
            "Epoch 3 Batch 450 Loss 0.5729\n",
            "Epoch 3 Batch 460 Loss 0.5576\n",
            "Epoch 3 Batch 470 Loss 0.5225\n",
            "Epoch 3 Batch 480 Loss 0.5037\n",
            "Epoch 3 Batch 490 Loss 0.5799\n",
            "Epoch 3 Batch 500 Loss 0.5332\n",
            "Epoch 3 Batch 510 Loss 0.4274\n",
            "Epoch 3 Batch 520 Loss 0.5185\n",
            "Epoch 3 Batch 530 Loss 0.4313\n",
            "Epoch 3 Batch 540 Loss 0.4497\n",
            "Epoch 3 Loss 0.5362\n",
            "Time taken for 1 epoch 162.91430187225342 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.4389\n",
            "Epoch 4 Batch 10 Loss 0.3873\n",
            "Epoch 4 Batch 20 Loss 0.4615\n",
            "Epoch 4 Batch 30 Loss 0.4673\n",
            "Epoch 4 Batch 40 Loss 0.3556\n",
            "Epoch 4 Batch 50 Loss 0.4504\n",
            "Epoch 4 Batch 60 Loss 0.4191\n",
            "Epoch 4 Batch 70 Loss 0.4053\n",
            "Epoch 4 Batch 80 Loss 0.3784\n",
            "Epoch 4 Batch 90 Loss 0.4062\n",
            "Epoch 4 Batch 100 Loss 0.3109\n",
            "Epoch 4 Batch 110 Loss 0.4242\n",
            "Epoch 4 Batch 120 Loss 0.4391\n",
            "Epoch 4 Batch 130 Loss 0.4932\n",
            "Epoch 4 Batch 140 Loss 0.3314\n",
            "Epoch 4 Batch 150 Loss 0.4026\n",
            "Epoch 4 Batch 160 Loss 0.4226\n",
            "Epoch 4 Batch 170 Loss 0.3614\n",
            "Epoch 4 Batch 180 Loss 0.4392\n",
            "Epoch 4 Batch 190 Loss 0.3440\n",
            "Epoch 4 Batch 200 Loss 0.3450\n",
            "Epoch 4 Batch 210 Loss 0.4121\n",
            "Epoch 4 Batch 220 Loss 0.3832\n",
            "Epoch 4 Batch 230 Loss 0.4485\n",
            "Epoch 4 Batch 240 Loss 0.3802\n",
            "Epoch 4 Batch 250 Loss 0.5076\n",
            "Epoch 4 Batch 260 Loss 0.3522\n",
            "Epoch 4 Batch 270 Loss 0.3241\n",
            "Epoch 4 Batch 280 Loss 0.3791\n",
            "Epoch 4 Batch 290 Loss 0.3631\n",
            "Epoch 4 Batch 300 Loss 0.4222\n",
            "Epoch 4 Batch 310 Loss 0.3923\n",
            "Epoch 4 Batch 320 Loss 0.3440\n",
            "Epoch 4 Batch 330 Loss 0.3992\n",
            "Epoch 4 Batch 340 Loss 0.3889\n",
            "Epoch 4 Batch 350 Loss 0.4231\n",
            "Epoch 4 Batch 360 Loss 0.4049\n",
            "Epoch 4 Batch 370 Loss 0.3710\n",
            "Epoch 4 Batch 380 Loss 0.3976\n",
            "Epoch 4 Batch 390 Loss 0.3849\n",
            "Epoch 4 Batch 400 Loss 0.3584\n",
            "Epoch 4 Batch 410 Loss 0.3282\n",
            "Epoch 4 Batch 420 Loss 0.4032\n",
            "Epoch 4 Batch 430 Loss 0.3765\n",
            "Epoch 4 Batch 440 Loss 0.3719\n",
            "Epoch 4 Batch 450 Loss 0.3107\n",
            "Epoch 4 Batch 460 Loss 0.3747\n",
            "Epoch 4 Batch 470 Loss 0.4532\n",
            "Epoch 4 Batch 480 Loss 0.3428\n",
            "Epoch 4 Batch 490 Loss 0.3866\n",
            "Epoch 4 Batch 500 Loss 0.3097\n",
            "Epoch 4 Batch 510 Loss 0.3983\n",
            "Epoch 4 Batch 520 Loss 0.3276\n",
            "Epoch 4 Batch 530 Loss 0.3403\n",
            "Epoch 4 Batch 540 Loss 0.3572\n",
            "Epoch 4 Loss 0.4008\n",
            "Time taken for 1 epoch 162.47682428359985 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.3091\n",
            "Epoch 5 Batch 10 Loss 0.2551\n",
            "Epoch 5 Batch 20 Loss 0.3710\n",
            "Epoch 5 Batch 30 Loss 0.2577\n",
            "Epoch 5 Batch 40 Loss 0.2521\n",
            "Epoch 5 Batch 50 Loss 0.2798\n",
            "Epoch 5 Batch 60 Loss 0.3373\n",
            "Epoch 5 Batch 70 Loss 0.3307\n",
            "Epoch 5 Batch 80 Loss 0.2686\n",
            "Epoch 5 Batch 90 Loss 0.2128\n",
            "Epoch 5 Batch 100 Loss 0.3327\n",
            "Epoch 5 Batch 110 Loss 0.2222\n",
            "Epoch 5 Batch 120 Loss 0.2739\n",
            "Epoch 5 Batch 130 Loss 0.2753\n",
            "Epoch 5 Batch 140 Loss 0.3055\n",
            "Epoch 5 Batch 150 Loss 0.3016\n",
            "Epoch 5 Batch 160 Loss 0.2568\n",
            "Epoch 5 Batch 170 Loss 0.3933\n",
            "Epoch 5 Batch 180 Loss 0.2793\n",
            "Epoch 5 Batch 190 Loss 0.2641\n",
            "Epoch 5 Batch 200 Loss 0.2572\n",
            "Epoch 5 Batch 210 Loss 0.3303\n",
            "Epoch 5 Batch 220 Loss 0.2839\n",
            "Epoch 5 Batch 230 Loss 0.3208\n",
            "Epoch 5 Batch 240 Loss 0.2868\n",
            "Epoch 5 Batch 250 Loss 0.2752\n",
            "Epoch 5 Batch 260 Loss 0.2721\n",
            "Epoch 5 Batch 270 Loss 0.3874\n",
            "Epoch 5 Batch 280 Loss 0.2615\n",
            "Epoch 5 Batch 290 Loss 0.3587\n",
            "Epoch 5 Batch 300 Loss 0.2618\n",
            "Epoch 5 Batch 310 Loss 0.3423\n",
            "Epoch 5 Batch 320 Loss 0.3226\n",
            "Epoch 5 Batch 330 Loss 0.2845\n",
            "Epoch 5 Batch 340 Loss 0.2641\n",
            "Epoch 5 Batch 350 Loss 0.3168\n",
            "Epoch 5 Batch 360 Loss 0.3008\n",
            "Epoch 5 Batch 370 Loss 0.3496\n",
            "Epoch 5 Batch 380 Loss 0.3065\n",
            "Epoch 5 Batch 390 Loss 0.2866\n",
            "Epoch 5 Batch 400 Loss 0.3482\n",
            "Epoch 5 Batch 410 Loss 0.2640\n",
            "Epoch 5 Batch 420 Loss 0.2412\n",
            "Epoch 5 Batch 430 Loss 0.3114\n",
            "Epoch 5 Batch 440 Loss 0.2867\n",
            "Epoch 5 Batch 450 Loss 0.2603\n",
            "Epoch 5 Batch 460 Loss 0.3002\n",
            "Epoch 5 Batch 470 Loss 0.2799\n",
            "Epoch 5 Batch 480 Loss 0.2592\n",
            "Epoch 5 Batch 490 Loss 0.2490\n",
            "Epoch 5 Batch 500 Loss 0.3206\n",
            "Epoch 5 Batch 510 Loss 0.2516\n",
            "Epoch 5 Batch 520 Loss 0.3155\n",
            "Epoch 5 Batch 530 Loss 0.2969\n",
            "Epoch 5 Batch 540 Loss 0.2645\n",
            "Epoch 5 Loss 0.2942\n",
            "Time taken for 1 epoch 162.28674173355103 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1951\n",
            "Epoch 6 Batch 10 Loss 0.2233\n",
            "Epoch 6 Batch 20 Loss 0.1379\n",
            "Epoch 6 Batch 30 Loss 0.2173\n",
            "Epoch 6 Batch 40 Loss 0.1484\n",
            "Epoch 6 Batch 50 Loss 0.2566\n",
            "Epoch 6 Batch 60 Loss 0.1315\n",
            "Epoch 6 Batch 70 Loss 0.2317\n",
            "Epoch 6 Batch 80 Loss 0.1715\n",
            "Epoch 6 Batch 90 Loss 0.2379\n",
            "Epoch 6 Batch 100 Loss 0.2041\n",
            "Epoch 6 Batch 110 Loss 0.1464\n",
            "Epoch 6 Batch 120 Loss 0.3121\n",
            "Epoch 6 Batch 130 Loss 0.2553\n",
            "Epoch 6 Batch 140 Loss 0.2308\n",
            "Epoch 6 Batch 150 Loss 0.2102\n",
            "Epoch 6 Batch 160 Loss 0.1564\n",
            "Epoch 6 Batch 170 Loss 0.1531\n",
            "Epoch 6 Batch 180 Loss 0.2170\n",
            "Epoch 6 Batch 190 Loss 0.2358\n",
            "Epoch 6 Batch 200 Loss 0.1597\n",
            "Epoch 6 Batch 210 Loss 0.2264\n",
            "Epoch 6 Batch 220 Loss 0.1619\n",
            "Epoch 6 Batch 230 Loss 0.2318\n",
            "Epoch 6 Batch 240 Loss 0.1933\n",
            "Epoch 6 Batch 250 Loss 0.2247\n",
            "Epoch 6 Batch 260 Loss 0.2211\n",
            "Epoch 6 Batch 270 Loss 0.2490\n",
            "Epoch 6 Batch 280 Loss 0.2439\n",
            "Epoch 6 Batch 290 Loss 0.2058\n",
            "Epoch 6 Batch 300 Loss 0.2218\n",
            "Epoch 6 Batch 310 Loss 0.2564\n",
            "Epoch 6 Batch 320 Loss 0.1919\n",
            "Epoch 6 Batch 330 Loss 0.1961\n",
            "Epoch 6 Batch 340 Loss 0.2023\n",
            "Epoch 6 Batch 350 Loss 0.1966\n",
            "Epoch 6 Batch 360 Loss 0.1868\n",
            "Epoch 6 Batch 370 Loss 0.1861\n",
            "Epoch 6 Batch 380 Loss 0.2284\n",
            "Epoch 6 Batch 390 Loss 0.2561\n",
            "Epoch 6 Batch 400 Loss 0.2564\n",
            "Epoch 6 Batch 410 Loss 0.2093\n",
            "Epoch 6 Batch 420 Loss 0.2292\n",
            "Epoch 6 Batch 430 Loss 0.2182\n",
            "Epoch 6 Batch 440 Loss 0.1884\n",
            "Epoch 6 Batch 450 Loss 0.2473\n",
            "Epoch 6 Batch 460 Loss 0.1751\n",
            "Epoch 6 Batch 470 Loss 0.2215\n",
            "Epoch 6 Batch 480 Loss 0.2299\n",
            "Epoch 6 Batch 490 Loss 0.2244\n",
            "Epoch 6 Batch 500 Loss 0.1840\n",
            "Epoch 6 Batch 510 Loss 0.1948\n",
            "Epoch 6 Batch 520 Loss 0.2216\n",
            "Epoch 6 Batch 530 Loss 0.2359\n",
            "Epoch 6 Batch 540 Loss 0.1982\n",
            "Epoch 6 Loss 0.2158\n",
            "Time taken for 1 epoch 162.300612449646 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1491\n",
            "Epoch 7 Batch 10 Loss 0.1567\n",
            "Epoch 7 Batch 20 Loss 0.1453\n",
            "Epoch 7 Batch 30 Loss 0.1345\n",
            "Epoch 7 Batch 40 Loss 0.1545\n",
            "Epoch 7 Batch 50 Loss 0.1071\n",
            "Epoch 7 Batch 60 Loss 0.1884\n",
            "Epoch 7 Batch 70 Loss 0.1463\n",
            "Epoch 7 Batch 80 Loss 0.1471\n",
            "Epoch 7 Batch 90 Loss 0.1857\n",
            "Epoch 7 Batch 100 Loss 0.1476\n",
            "Epoch 7 Batch 110 Loss 0.1543\n",
            "Epoch 7 Batch 120 Loss 0.1529\n",
            "Epoch 7 Batch 130 Loss 0.1941\n",
            "Epoch 7 Batch 140 Loss 0.1355\n",
            "Epoch 7 Batch 150 Loss 0.1389\n",
            "Epoch 7 Batch 160 Loss 0.1615\n",
            "Epoch 7 Batch 170 Loss 0.1450\n",
            "Epoch 7 Batch 180 Loss 0.1666\n",
            "Epoch 7 Batch 190 Loss 0.1221\n",
            "Epoch 7 Batch 200 Loss 0.1710\n",
            "Epoch 7 Batch 210 Loss 0.1517\n",
            "Epoch 7 Batch 220 Loss 0.1570\n",
            "Epoch 7 Batch 230 Loss 0.1672\n",
            "Epoch 7 Batch 240 Loss 0.1599\n",
            "Epoch 7 Batch 250 Loss 0.1974\n",
            "Epoch 7 Batch 260 Loss 0.1376\n",
            "Epoch 7 Batch 270 Loss 0.1280\n",
            "Epoch 7 Batch 280 Loss 0.1682\n",
            "Epoch 7 Batch 290 Loss 0.1830\n",
            "Epoch 7 Batch 300 Loss 0.2434\n",
            "Epoch 7 Batch 310 Loss 0.1406\n",
            "Epoch 7 Batch 320 Loss 0.1710\n",
            "Epoch 7 Batch 330 Loss 0.1449\n",
            "Epoch 7 Batch 340 Loss 0.1798\n",
            "Epoch 7 Batch 350 Loss 0.1338\n",
            "Epoch 7 Batch 360 Loss 0.1529\n",
            "Epoch 7 Batch 370 Loss 0.1712\n",
            "Epoch 7 Batch 380 Loss 0.1606\n",
            "Epoch 7 Batch 390 Loss 0.1656\n",
            "Epoch 7 Batch 400 Loss 0.1530\n",
            "Epoch 7 Batch 410 Loss 0.1552\n",
            "Epoch 7 Batch 420 Loss 0.1728\n",
            "Epoch 7 Batch 430 Loss 0.1785\n",
            "Epoch 7 Batch 440 Loss 0.1575\n",
            "Epoch 7 Batch 450 Loss 0.1881\n",
            "Epoch 7 Batch 460 Loss 0.1192\n",
            "Epoch 7 Batch 470 Loss 0.1878\n",
            "Epoch 7 Batch 480 Loss 0.1867\n",
            "Epoch 7 Batch 490 Loss 0.1962\n",
            "Epoch 7 Batch 500 Loss 0.1263\n",
            "Epoch 7 Batch 510 Loss 0.1488\n",
            "Epoch 7 Batch 520 Loss 0.1513\n",
            "Epoch 7 Batch 530 Loss 0.1469\n",
            "Epoch 7 Batch 540 Loss 0.1530\n",
            "Epoch 7 Loss 0.1596\n",
            "Time taken for 1 epoch 162.36048221588135 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0958\n",
            "Epoch 8 Batch 10 Loss 0.0837\n",
            "Epoch 8 Batch 20 Loss 0.1005\n",
            "Epoch 8 Batch 30 Loss 0.1055\n",
            "Epoch 8 Batch 40 Loss 0.0945\n",
            "Epoch 8 Batch 50 Loss 0.0732\n",
            "Epoch 8 Batch 60 Loss 0.0779\n",
            "Epoch 8 Batch 70 Loss 0.1005\n",
            "Epoch 8 Batch 80 Loss 0.1236\n",
            "Epoch 8 Batch 90 Loss 0.0976\n",
            "Epoch 8 Batch 100 Loss 0.1673\n",
            "Epoch 8 Batch 110 Loss 0.1235\n",
            "Epoch 8 Batch 120 Loss 0.1149\n",
            "Epoch 8 Batch 130 Loss 0.0948\n",
            "Epoch 8 Batch 140 Loss 0.1067\n",
            "Epoch 8 Batch 150 Loss 0.1174\n",
            "Epoch 8 Batch 160 Loss 0.0948\n",
            "Epoch 8 Batch 170 Loss 0.0996\n",
            "Epoch 8 Batch 180 Loss 0.1244\n",
            "Epoch 8 Batch 190 Loss 0.0879\n",
            "Epoch 8 Batch 200 Loss 0.1623\n",
            "Epoch 8 Batch 210 Loss 0.1095\n",
            "Epoch 8 Batch 220 Loss 0.1141\n",
            "Epoch 8 Batch 230 Loss 0.1556\n",
            "Epoch 8 Batch 240 Loss 0.1078\n",
            "Epoch 8 Batch 250 Loss 0.1319\n",
            "Epoch 8 Batch 260 Loss 0.1601\n",
            "Epoch 8 Batch 270 Loss 0.1357\n",
            "Epoch 8 Batch 280 Loss 0.1384\n",
            "Epoch 8 Batch 290 Loss 0.0942\n",
            "Epoch 8 Batch 300 Loss 0.1032\n",
            "Epoch 8 Batch 310 Loss 0.1783\n",
            "Epoch 8 Batch 320 Loss 0.1270\n",
            "Epoch 8 Batch 330 Loss 0.1334\n",
            "Epoch 8 Batch 340 Loss 0.1203\n",
            "Epoch 8 Batch 350 Loss 0.1738\n",
            "Epoch 8 Batch 360 Loss 0.1137\n",
            "Epoch 8 Batch 370 Loss 0.1015\n",
            "Epoch 8 Batch 380 Loss 0.1243\n",
            "Epoch 8 Batch 390 Loss 0.1311\n",
            "Epoch 8 Batch 400 Loss 0.1456\n",
            "Epoch 8 Batch 410 Loss 0.1330\n",
            "Epoch 8 Batch 420 Loss 0.1053\n",
            "Epoch 8 Batch 430 Loss 0.1359\n",
            "Epoch 8 Batch 440 Loss 0.1735\n",
            "Epoch 8 Batch 450 Loss 0.1317\n",
            "Epoch 8 Batch 460 Loss 0.1104\n",
            "Epoch 8 Batch 470 Loss 0.1325\n",
            "Epoch 8 Batch 480 Loss 0.1433\n",
            "Epoch 8 Batch 490 Loss 0.1738\n",
            "Epoch 8 Batch 500 Loss 0.1315\n",
            "Epoch 8 Batch 510 Loss 0.0983\n",
            "Epoch 8 Batch 520 Loss 0.1176\n",
            "Epoch 8 Batch 530 Loss 0.1572\n",
            "Epoch 8 Batch 540 Loss 0.1362\n",
            "Epoch 8 Loss 0.1189\n",
            "Time taken for 1 epoch 162.179856300354 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0861\n",
            "Epoch 9 Batch 10 Loss 0.1018\n",
            "Epoch 9 Batch 20 Loss 0.0966\n",
            "Epoch 9 Batch 30 Loss 0.0874\n",
            "Epoch 9 Batch 40 Loss 0.0900\n",
            "Epoch 9 Batch 50 Loss 0.0903\n",
            "Epoch 9 Batch 60 Loss 0.0745\n",
            "Epoch 9 Batch 70 Loss 0.1042\n",
            "Epoch 9 Batch 80 Loss 0.0793\n",
            "Epoch 9 Batch 90 Loss 0.1265\n",
            "Epoch 9 Batch 100 Loss 0.0952\n",
            "Epoch 9 Batch 110 Loss 0.0728\n",
            "Epoch 9 Batch 120 Loss 0.0692\n",
            "Epoch 9 Batch 130 Loss 0.0706\n",
            "Epoch 9 Batch 140 Loss 0.0798\n",
            "Epoch 9 Batch 150 Loss 0.0874\n",
            "Epoch 9 Batch 160 Loss 0.0674\n",
            "Epoch 9 Batch 170 Loss 0.0614\n",
            "Epoch 9 Batch 180 Loss 0.1023\n",
            "Epoch 9 Batch 190 Loss 0.0728\n",
            "Epoch 9 Batch 200 Loss 0.0895\n",
            "Epoch 9 Batch 210 Loss 0.1057\n",
            "Epoch 9 Batch 220 Loss 0.0934\n",
            "Epoch 9 Batch 230 Loss 0.0615\n",
            "Epoch 9 Batch 240 Loss 0.0677\n",
            "Epoch 9 Batch 250 Loss 0.0768\n",
            "Epoch 9 Batch 260 Loss 0.0782\n",
            "Epoch 9 Batch 270 Loss 0.0892\n",
            "Epoch 9 Batch 280 Loss 0.1066\n",
            "Epoch 9 Batch 290 Loss 0.0876\n",
            "Epoch 9 Batch 300 Loss 0.0676\n",
            "Epoch 9 Batch 310 Loss 0.0964\n",
            "Epoch 9 Batch 320 Loss 0.1111\n",
            "Epoch 9 Batch 330 Loss 0.0649\n",
            "Epoch 9 Batch 340 Loss 0.0788\n",
            "Epoch 9 Batch 350 Loss 0.1050\n",
            "Epoch 9 Batch 360 Loss 0.0854\n",
            "Epoch 9 Batch 370 Loss 0.0891\n",
            "Epoch 9 Batch 380 Loss 0.0831\n",
            "Epoch 9 Batch 390 Loss 0.1027\n",
            "Epoch 9 Batch 400 Loss 0.1085\n",
            "Epoch 9 Batch 410 Loss 0.1208\n",
            "Epoch 9 Batch 420 Loss 0.0930\n",
            "Epoch 9 Batch 430 Loss 0.0930\n",
            "Epoch 9 Batch 440 Loss 0.0934\n",
            "Epoch 9 Batch 450 Loss 0.1279\n",
            "Epoch 9 Batch 460 Loss 0.0794\n",
            "Epoch 9 Batch 470 Loss 0.0806\n",
            "Epoch 9 Batch 480 Loss 0.1184\n",
            "Epoch 9 Batch 490 Loss 0.1387\n",
            "Epoch 9 Batch 500 Loss 0.0694\n",
            "Epoch 9 Batch 510 Loss 0.0769\n",
            "Epoch 9 Batch 520 Loss 0.1093\n",
            "Epoch 9 Batch 530 Loss 0.0846\n",
            "Epoch 9 Batch 540 Loss 0.0903\n",
            "Epoch 9 Loss 0.0907\n",
            "Time taken for 1 epoch 162.4392125606537 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0731\n",
            "Epoch 10 Batch 10 Loss 0.0548\n",
            "Epoch 10 Batch 20 Loss 0.0943\n",
            "Epoch 10 Batch 30 Loss 0.0648\n",
            "Epoch 10 Batch 40 Loss 0.0838\n",
            "Epoch 10 Batch 50 Loss 0.0620\n",
            "Epoch 10 Batch 60 Loss 0.0755\n",
            "Epoch 10 Batch 70 Loss 0.0792\n",
            "Epoch 10 Batch 80 Loss 0.0713\n",
            "Epoch 10 Batch 90 Loss 0.0426\n",
            "Epoch 10 Batch 100 Loss 0.0802\n",
            "Epoch 10 Batch 110 Loss 0.0565\n",
            "Epoch 10 Batch 120 Loss 0.0485\n",
            "Epoch 10 Batch 130 Loss 0.0488\n",
            "Epoch 10 Batch 140 Loss 0.0654\n",
            "Epoch 10 Batch 150 Loss 0.0396\n",
            "Epoch 10 Batch 160 Loss 0.0507\n",
            "Epoch 10 Batch 170 Loss 0.0697\n",
            "Epoch 10 Batch 180 Loss 0.0669\n",
            "Epoch 10 Batch 190 Loss 0.0468\n",
            "Epoch 10 Batch 200 Loss 0.0788\n",
            "Epoch 10 Batch 210 Loss 0.0766\n",
            "Epoch 10 Batch 220 Loss 0.0792\n",
            "Epoch 10 Batch 230 Loss 0.0676\n",
            "Epoch 10 Batch 240 Loss 0.0871\n",
            "Epoch 10 Batch 250 Loss 0.0731\n",
            "Epoch 10 Batch 260 Loss 0.0683\n",
            "Epoch 10 Batch 270 Loss 0.0649\n",
            "Epoch 10 Batch 280 Loss 0.0585\n",
            "Epoch 10 Batch 290 Loss 0.0663\n",
            "Epoch 10 Batch 300 Loss 0.0612\n",
            "Epoch 10 Batch 310 Loss 0.0741\n",
            "Epoch 10 Batch 320 Loss 0.1360\n",
            "Epoch 10 Batch 330 Loss 0.0871\n",
            "Epoch 10 Batch 340 Loss 0.0559\n",
            "Epoch 10 Batch 350 Loss 0.0560\n",
            "Epoch 10 Batch 360 Loss 0.0558\n",
            "Epoch 10 Batch 370 Loss 0.0849\n",
            "Epoch 10 Batch 380 Loss 0.0809\n",
            "Epoch 10 Batch 390 Loss 0.0556\n",
            "Epoch 10 Batch 400 Loss 0.0669\n",
            "Epoch 10 Batch 410 Loss 0.0590\n",
            "Epoch 10 Batch 420 Loss 0.0766\n",
            "Epoch 10 Batch 430 Loss 0.1137\n",
            "Epoch 10 Batch 440 Loss 0.0706\n",
            "Epoch 10 Batch 450 Loss 0.0793\n",
            "Epoch 10 Batch 460 Loss 0.0721\n",
            "Epoch 10 Batch 470 Loss 0.0869\n",
            "Epoch 10 Batch 480 Loss 0.0665\n",
            "Epoch 10 Batch 490 Loss 0.0930\n",
            "Epoch 10 Batch 500 Loss 0.0711\n",
            "Epoch 10 Batch 510 Loss 0.0724\n",
            "Epoch 10 Batch 520 Loss 0.0576\n",
            "Epoch 10 Batch 530 Loss 0.0671\n",
            "Epoch 10 Batch 540 Loss 0.0668\n",
            "Epoch 10 Loss 0.0695\n",
            "Time taken for 1 epoch 162.4010875225067 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLK_47Bg_iU2"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "encoder_latest=checkpoint.encoder\n",
        "decoder_latest=checkpoint.decoder"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_2bS3Cm_aDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8199df6-1633-4151-b6ce-9b82c548407f"
      },
      "source": [
        "for inp, targ in test_data.take(1):\n",
        "  print(\"input sentence : {}\".format(tokenizer_fr.sequences_to_texts(inp.numpy())[0]))\n",
        "  print(\"target sentence : {}\".format(tokenizer_en.sequences_to_texts(targ.numpy())[0]))\n",
        "  enc_hidden = encoder_latest.initialize_hidden_state()\n",
        "  enc_output, enc_hidden = encoder_latest(inp, enc_hidden)\n",
        "\n",
        "  # tensor containing the first token of target :  <start>\n",
        "  result = tf.expand_dims(tokenizer_en.sequences_to_texts([[index] for index in targ[:,0].numpy()]),1)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "\n",
        "  dec_input = tf.expand_dims(targ[:,0], 1)\n",
        "\n",
        "  # Teacher forcing - feeding the target as the next input\n",
        "  for t in range(1, targ.shape[1]):\n",
        "    # passing enc_output to the decoder\n",
        "    predictions, dec_hidden, _ = decoder_latest(dec_input, dec_hidden, enc_output)\n",
        "    \n",
        "\n",
        "    # get text predictions\n",
        "    pred_index = tf.argmax(predictions, axis = 1).numpy()\n",
        "    corresponding_word = tf.expand_dims(tokenizer_en.sequences_to_texts([[index] for index in pred_index]),1)\n",
        "    result = tf.concat((result,corresponding_word), axis=1)\n",
        "\n",
        "    # using teacher forcing\n",
        "    dec_input = tf.expand_dims(pred_index,1)\n",
        "\n",
        "  result = [\" \".join([word.decode(\"utf-8\") for word in sentence]) for sentence in result.numpy()]\n",
        "  print(result[0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input sentence : <start> avez vous réglé la note <end>\n",
            "target sentence : <start> did you settle the bill <end>\n",
            "<start> did you open the instructions <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZMyM88_JPs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}